{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\whtwht97\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ppb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#tqdm.pandas(desc='pandas bar')\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from gensim.models import Word2Vec\n",
    "import gc\n",
    "import time\n",
    "\n",
    "#general purpose packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#data processing\n",
    "import re, string\n",
    "import nltk\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#lightgbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "#transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import TFRobertaModel\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#tqdm.pandas(desc='pandas bar')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-cased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"drugsComTrain_raw.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = pd.read_csv(\"drugsComTest_raw.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1e65ffffdf34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Truncate long sentences to 128 tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#del df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4432\u001b[0m         \"\"\"\n\u001b[1;32m-> 4433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1135\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1137\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1138\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-1e65ffffdf34>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Truncate long sentences to 128 tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#del df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   2224\u001b[0m                 method).\n\u001b[0;32m   2225\u001b[0m         \"\"\"\n\u001b[1;32m-> 2226\u001b[1;33m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[0;32m   2227\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2554\u001b[0m         )\n\u001b[0;32m   2555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2556\u001b[1;33m         return self._encode_plus(\n\u001b[0m\u001b[0;32m   2557\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2558\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    645\u001b[0m             )\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m         \u001b[0mfirst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    632\u001b[0m                     )\n\u001b[0;32m    633\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    635\u001b[0m                         \u001b[1;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "# Truncate long sentences to 128 tokens\n",
    "X = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128, truncation=True)))\n",
    "y = np.array(df[1])\n",
    "#del df\n",
    "\n",
    "X_test = test_df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128, truncation=True)))\n",
    "y_test = np.array(test_df[1])\n",
    "#del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding of y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "\n",
    "y = encoder.transform(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# One hot Encoding of y test\n",
    "y_oh = encoder.transform(y_test)\n",
    "y_oh = to_categorical(y_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmbeddings(tokenizedBatch):\n",
    "    max_len = 0\n",
    "    for i in tokenizedBatch.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenizedBatch.values])\n",
    "    \n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    input_ids = torch.tensor(padded).to(torch.long)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings for Batch: 1 of 162\n",
      "Generating Embeddings for Batch: 2 of 162\n",
      "Generating Embeddings for Batch: 3 of 162\n",
      "Generating Embeddings for Batch: 4 of 162\n",
      "Generating Embeddings for Batch: 5 of 162\n",
      "Generating Embeddings for Batch: 6 of 162\n",
      "Generating Embeddings for Batch: 7 of 162\n",
      "Generating Embeddings for Batch: 8 of 162\n",
      "Generating Embeddings for Batch: 9 of 162\n",
      "Generating Embeddings for Batch: 10 of 162\n",
      "Generating Embeddings for Batch: 11 of 162\n",
      "Generating Embeddings for Batch: 12 of 162\n",
      "Generating Embeddings for Batch: 13 of 162\n",
      "Generating Embeddings for Batch: 14 of 162\n",
      "Generating Embeddings for Batch: 15 of 162\n",
      "Generating Embeddings for Batch: 16 of 162\n",
      "Generating Embeddings for Batch: 17 of 162\n",
      "Generating Embeddings for Batch: 18 of 162\n",
      "Generating Embeddings for Batch: 19 of 162\n",
      "Generating Embeddings for Batch: 20 of 162\n",
      "Generating Embeddings for Batch: 21 of 162\n",
      "Generating Embeddings for Batch: 22 of 162\n",
      "Generating Embeddings for Batch: 23 of 162\n",
      "Generating Embeddings for Batch: 24 of 162\n",
      "Generating Embeddings for Batch: 25 of 162\n",
      "Generating Embeddings for Batch: 26 of 162\n",
      "Generating Embeddings for Batch: 27 of 162\n",
      "Generating Embeddings for Batch: 28 of 162\n",
      "Generating Embeddings for Batch: 29 of 162\n",
      "Generating Embeddings for Batch: 30 of 162\n",
      "Generating Embeddings for Batch: 31 of 162\n",
      "Generating Embeddings for Batch: 32 of 162\n",
      "Generating Embeddings for Batch: 33 of 162\n",
      "Generating Embeddings for Batch: 34 of 162\n",
      "Generating Embeddings for Batch: 35 of 162\n",
      "Generating Embeddings for Batch: 36 of 162\n",
      "Generating Embeddings for Batch: 37 of 162\n",
      "Generating Embeddings for Batch: 38 of 162\n",
      "Generating Embeddings for Batch: 39 of 162\n",
      "Generating Embeddings for Batch: 40 of 162\n",
      "Generating Embeddings for Batch: 41 of 162\n",
      "Generating Embeddings for Batch: 42 of 162\n",
      "Generating Embeddings for Batch: 43 of 162\n",
      "Generating Embeddings for Batch: 44 of 162\n",
      "Generating Embeddings for Batch: 45 of 162\n",
      "Generating Embeddings for Batch: 46 of 162\n",
      "Generating Embeddings for Batch: 47 of 162\n",
      "Generating Embeddings for Batch: 48 of 162\n",
      "Generating Embeddings for Batch: 49 of 162\n",
      "Generating Embeddings for Batch: 50 of 162\n",
      "Generating Embeddings for Batch: 51 of 162\n",
      "Generating Embeddings for Batch: 52 of 162\n",
      "Generating Embeddings for Batch: 53 of 162\n",
      "Generating Embeddings for Batch: 54 of 162\n",
      "Generating Embeddings for Batch: 55 of 162\n",
      "Generating Embeddings for Batch: 56 of 162\n",
      "Generating Embeddings for Batch: 57 of 162\n",
      "Generating Embeddings for Batch: 58 of 162\n",
      "Generating Embeddings for Batch: 59 of 162\n",
      "Generating Embeddings for Batch: 60 of 162\n",
      "Generating Embeddings for Batch: 61 of 162\n",
      "Generating Embeddings for Batch: 62 of 162\n",
      "Generating Embeddings for Batch: 63 of 162\n",
      "Generating Embeddings for Batch: 64 of 162\n",
      "Generating Embeddings for Batch: 65 of 162\n",
      "Generating Embeddings for Batch: 66 of 162\n",
      "Generating Embeddings for Batch: 67 of 162\n",
      "Generating Embeddings for Batch: 68 of 162\n",
      "Generating Embeddings for Batch: 69 of 162\n",
      "Generating Embeddings for Batch: 70 of 162\n",
      "Generating Embeddings for Batch: 71 of 162\n",
      "Generating Embeddings for Batch: 72 of 162\n",
      "Generating Embeddings for Batch: 73 of 162\n",
      "Generating Embeddings for Batch: 74 of 162\n",
      "Generating Embeddings for Batch: 75 of 162\n",
      "Generating Embeddings for Batch: 76 of 162\n",
      "Generating Embeddings for Batch: 77 of 162\n",
      "Generating Embeddings for Batch: 78 of 162\n",
      "Generating Embeddings for Batch: 79 of 162\n",
      "Generating Embeddings for Batch: 80 of 162\n",
      "Generating Embeddings for Batch: 81 of 162\n",
      "Generating Embeddings for Batch: 82 of 162\n",
      "Generating Embeddings for Batch: 83 of 162\n",
      "Generating Embeddings for Batch: 84 of 162\n",
      "Generating Embeddings for Batch: 85 of 162\n",
      "Generating Embeddings for Batch: 86 of 162\n",
      "Generating Embeddings for Batch: 87 of 162\n",
      "Generating Embeddings for Batch: 88 of 162\n",
      "Generating Embeddings for Batch: 89 of 162\n",
      "Generating Embeddings for Batch: 90 of 162\n",
      "Generating Embeddings for Batch: 91 of 162\n",
      "Generating Embeddings for Batch: 92 of 162\n",
      "Generating Embeddings for Batch: 93 of 162\n",
      "Generating Embeddings for Batch: 94 of 162\n",
      "Generating Embeddings for Batch: 95 of 162\n",
      "Generating Embeddings for Batch: 96 of 162\n",
      "Generating Embeddings for Batch: 97 of 162\n",
      "Generating Embeddings for Batch: 98 of 162\n",
      "Generating Embeddings for Batch: 99 of 162\n",
      "Generating Embeddings for Batch: 100 of 162\n",
      "Generating Embeddings for Batch: 101 of 162\n",
      "Generating Embeddings for Batch: 102 of 162\n",
      "Generating Embeddings for Batch: 103 of 162\n",
      "Generating Embeddings for Batch: 104 of 162\n",
      "Generating Embeddings for Batch: 105 of 162\n",
      "Generating Embeddings for Batch: 106 of 162\n",
      "Generating Embeddings for Batch: 107 of 162\n",
      "Generating Embeddings for Batch: 108 of 162\n",
      "Generating Embeddings for Batch: 109 of 162\n",
      "Generating Embeddings for Batch: 110 of 162\n",
      "Generating Embeddings for Batch: 111 of 162\n",
      "Generating Embeddings for Batch: 112 of 162\n",
      "Generating Embeddings for Batch: 113 of 162\n",
      "Generating Embeddings for Batch: 114 of 162\n",
      "Generating Embeddings for Batch: 115 of 162\n",
      "Generating Embeddings for Batch: 116 of 162\n",
      "Generating Embeddings for Batch: 117 of 162\n",
      "Generating Embeddings for Batch: 118 of 162\n",
      "Generating Embeddings for Batch: 119 of 162\n",
      "Generating Embeddings for Batch: 120 of 162\n",
      "Generating Embeddings for Batch: 121 of 162\n",
      "Generating Embeddings for Batch: 122 of 162\n",
      "Generating Embeddings for Batch: 123 of 162\n",
      "Generating Embeddings for Batch: 124 of 162\n",
      "Generating Embeddings for Batch: 125 of 162\n",
      "Generating Embeddings for Batch: 126 of 162\n",
      "Generating Embeddings for Batch: 127 of 162\n",
      "Generating Embeddings for Batch: 128 of 162\n",
      "Generating Embeddings for Batch: 129 of 162\n",
      "Generating Embeddings for Batch: 130 of 162\n",
      "Generating Embeddings for Batch: 131 of 162\n",
      "Generating Embeddings for Batch: 132 of 162\n",
      "Generating Embeddings for Batch: 133 of 162\n",
      "Generating Embeddings for Batch: 134 of 162\n",
      "Generating Embeddings for Batch: 135 of 162\n",
      "Generating Embeddings for Batch: 136 of 162\n",
      "Generating Embeddings for Batch: 137 of 162\n",
      "Generating Embeddings for Batch: 138 of 162\n",
      "Generating Embeddings for Batch: 139 of 162\n",
      "Generating Embeddings for Batch: 140 of 162\n",
      "Generating Embeddings for Batch: 141 of 162\n",
      "Generating Embeddings for Batch: 142 of 162\n",
      "Generating Embeddings for Batch: 143 of 162\n",
      "Generating Embeddings for Batch: 144 of 162\n",
      "Generating Embeddings for Batch: 145 of 162\n",
      "Generating Embeddings for Batch: 146 of 162\n",
      "Generating Embeddings for Batch: 147 of 162\n",
      "Generating Embeddings for Batch: 148 of 162\n",
      "Generating Embeddings for Batch: 149 of 162\n",
      "Generating Embeddings for Batch: 150 of 162\n",
      "Generating Embeddings for Batch: 151 of 162\n",
      "Generating Embeddings for Batch: 152 of 162\n",
      "Generating Embeddings for Batch: 153 of 162\n",
      "Generating Embeddings for Batch: 154 of 162\n",
      "Generating Embeddings for Batch: 155 of 162\n",
      "Generating Embeddings for Batch: 156 of 162\n",
      "Generating Embeddings for Batch: 157 of 162\n",
      "Generating Embeddings for Batch: 158 of 162\n",
      "Generating Embeddings for Batch: 159 of 162\n",
      "Generating Embeddings for Batch: 160 of 162\n",
      "Generating Embeddings for Batch: 161 of 162\n",
      "Generating Embeddings for Batch: 162 of 162\n",
      "Generating Test Embeddings for Batch: 1 of 54\n",
      "Generating Test Embeddings for Batch: 2 of 54\n",
      "Generating Test Embeddings for Batch: 3 of 54\n",
      "Generating Test Embeddings for Batch: 4 of 54\n",
      "Generating Test Embeddings for Batch: 5 of 54\n",
      "Generating Test Embeddings for Batch: 6 of 54\n",
      "Generating Test Embeddings for Batch: 7 of 54\n",
      "Generating Test Embeddings for Batch: 8 of 54\n",
      "Generating Test Embeddings for Batch: 9 of 54\n",
      "Generating Test Embeddings for Batch: 10 of 54\n",
      "Generating Test Embeddings for Batch: 11 of 54\n",
      "Generating Test Embeddings for Batch: 12 of 54\n",
      "Generating Test Embeddings for Batch: 13 of 54\n",
      "Generating Test Embeddings for Batch: 14 of 54\n",
      "Generating Test Embeddings for Batch: 15 of 54\n",
      "Generating Test Embeddings for Batch: 16 of 54\n",
      "Generating Test Embeddings for Batch: 17 of 54\n",
      "Generating Test Embeddings for Batch: 18 of 54\n",
      "Generating Test Embeddings for Batch: 19 of 54\n",
      "Generating Test Embeddings for Batch: 20 of 54\n",
      "Generating Test Embeddings for Batch: 21 of 54\n",
      "Generating Test Embeddings for Batch: 22 of 54\n",
      "Generating Test Embeddings for Batch: 23 of 54\n",
      "Generating Test Embeddings for Batch: 24 of 54\n",
      "Generating Test Embeddings for Batch: 25 of 54\n",
      "Generating Test Embeddings for Batch: 26 of 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Test Embeddings for Batch: 27 of 54\n",
      "Generating Test Embeddings for Batch: 28 of 54\n",
      "Generating Test Embeddings for Batch: 29 of 54\n",
      "Generating Test Embeddings for Batch: 30 of 54\n",
      "Generating Test Embeddings for Batch: 31 of 54\n",
      "Generating Test Embeddings for Batch: 32 of 54\n",
      "Generating Test Embeddings for Batch: 33 of 54\n",
      "Generating Test Embeddings for Batch: 34 of 54\n",
      "Generating Test Embeddings for Batch: 35 of 54\n",
      "Generating Test Embeddings for Batch: 36 of 54\n",
      "Generating Test Embeddings for Batch: 37 of 54\n",
      "Generating Test Embeddings for Batch: 38 of 54\n",
      "Generating Test Embeddings for Batch: 39 of 54\n",
      "Generating Test Embeddings for Batch: 40 of 54\n",
      "Generating Test Embeddings for Batch: 41 of 54\n",
      "Generating Test Embeddings for Batch: 42 of 54\n",
      "Generating Test Embeddings for Batch: 43 of 54\n",
      "Generating Test Embeddings for Batch: 44 of 54\n",
      "Generating Test Embeddings for Batch: 45 of 54\n",
      "Generating Test Embeddings for Batch: 46 of 54\n",
      "Generating Test Embeddings for Batch: 47 of 54\n",
      "Generating Test Embeddings for Batch: 48 of 54\n",
      "Generating Test Embeddings for Batch: 49 of 54\n",
      "Generating Test Embeddings for Batch: 50 of 54\n",
      "Generating Test Embeddings for Batch: 51 of 54\n",
      "Generating Test Embeddings for Batch: 52 of 54\n",
      "Generating Test Embeddings for Batch: 53 of 54\n",
      "Generating Test Embeddings for Batch: 54 of 54\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "all_embeddings = []\n",
    "all_embeddings_test = []\n",
    "\n",
    "# Process Training Set Embeddings\n",
    "batches = math.ceil(X.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for i in range(1, batches+1):\n",
    "    print(\"Generating Embeddings for Batch:\",i,\"of\", batches)\n",
    "    batchEmbeddings = GetEmbeddings(X[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "    all_embeddings.append(batchEmbeddings)\n",
    "\n",
    "# Process Test Set Embeddings\n",
    "batches = math.ceil(X_test.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for i in range(1, batches+1):\n",
    "    print(\"Generating Test Embeddings for Batch:\",i,\"of\", batches)\n",
    "    batchEmbeddings = GetEmbeddings(X_test[(i-1)*BATCH_SIZE:i*BATCH_SIZE])\n",
    "    all_embeddings_test.append(batchEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "all_embeddings_test = np.concatenate(all_embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bert_embeddings.npy', all_embeddings)\n",
    "np.save('y.npy', y)\n",
    "np.save('bert_embeddings_test.npy', all_embeddings_test)\n",
    "np.save('y_test.npy', y_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "#from keras.optimizers import adam, sgd\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.load('bert_embeddings.npy')\n",
    "y = np.load('y.npy')\n",
    "all_embeddings_test = np.load('bert_embeddings_test.npy')\n",
    "y_oh = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd = sgd(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optim = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    embedding = Input(shape=(768,), dtype=\"float\")\n",
    "    dense1 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(embedding)\n",
    "    dense2 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense1)\n",
    "    dense3 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense2)\n",
    "    dense4 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense3)\n",
    "    dense5 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense4)\n",
    "    dense6 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense5)\n",
    "    dense7 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense6)\n",
    "    dense8 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense7)\n",
    "    dense9 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense8)\n",
    "    dense10 = Dense(1000, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense9)\n",
    "    pred = Dense(3, activation='sigmoid')(dense9)\n",
    "    model = Model(inputs=[embedding], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'], )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 768)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              769000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3)                 3003      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,780,003\n",
      "Trainable params: 8,780,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', patience=50)\n",
    "cb_list = [es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    history = model_bert.fit(all_embeddings, y, epochs=10, batch_size=10000, \n",
    "                             validation_split = 0.001, callbacks=cb_list)\n",
    "    model_bert.save_weights('model_bert_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 22s 1s/step - loss: 7.7277 - accuracy: 0.5796 - val_loss: 5.4893 - val_accuracy: 0.5679\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 22s 1s/step - loss: 4.3168 - accuracy: 0.6040 - val_loss: 3.2651 - val_accuracy: 0.5679\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 24s 1s/step - loss: 2.7062 - accuracy: 0.6040 - val_loss: 2.2518 - val_accuracy: 0.5679\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 24s 1s/step - loss: 2.0077 - accuracy: 0.6401 - val_loss: 1.7940 - val_accuracy: 0.6728\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.6878 - accuracy: 0.6460 - val_loss: 1.5785 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.4790 - accuracy: 0.6621 - val_loss: 1.3813 - val_accuracy: 0.6790\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.3265 - accuracy: 0.6694 - val_loss: 1.3723 - val_accuracy: 0.6049\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.2486 - accuracy: 0.6608 - val_loss: 1.1747 - val_accuracy: 0.6852\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.1492 - accuracy: 0.6723 - val_loss: 1.0927 - val_accuracy: 0.6914\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.0979 - accuracy: 0.6691 - val_loss: 1.1185 - val_accuracy: 0.6852\n"
     ]
    }
   ],
   "source": [
    "history = model_bert.fit(all_embeddings, y, epochs=10, batch_size=10000, \n",
    "                             validation_split = 0.001, callbacks=cb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-2351184ea3d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training Acc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training Acc')\n",
    "plt.title('Training and validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10000\n",
    "batches = math.ceil(all_embeddings_test.shape[0] / bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_probs = []\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    model_bert.load_weights('model_bert_weights.h5')\n",
    "\n",
    "    for i in range(1,batches+1):\n",
    "        print(\"Predicting Batch\",i)\n",
    "        new_text_pr = all_embeddings_test[(i-1)*bs:i*bs]\n",
    "        preds = model_bert.predict(new_text_pr)\n",
    "        all_probs.append(preds)\n",
    "        preds = encoder.inverse_transform(np.argmax(preds,axis=1))\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.concatenate(all_preds, axis=0)\n",
    "results_probs = np.concatenate(all_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",sum(results==y_test)/results.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
